{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQtxM1rOL38i",
        "outputId": "a3ee1ee1-c86d-4b2a-8b03-6568605a4b2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Final Data Manifest Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 6424 entries, 0 to 10236\n",
            "Data columns (total 3 columns):\n",
            " #   Column             Non-Null Count  Dtype \n",
            "---  ------             --------------  ----- \n",
            " 0   full_image_path    6424 non-null   object\n",
            " 1   pathology          6424 non-null   object\n",
            " 2   SeriesDescription  6424 non-null   object\n",
            "dtypes: object(3)\n",
            "memory usage: 200.8+ KB\n",
            "None\n",
            "\n",
            "--- Value Counts for Pathology ---\n",
            "pathology\n",
            "MALIGNANT                  2725\n",
            "BENIGN                     2660\n",
            "BENIGN_WITHOUT_CALLBACK    1039\n",
            "Name: count, dtype: int64\n",
            "\n",
            "--- Value Counts for Image Type ---\n",
            "SeriesDescription\n",
            "cropped images           3567\n",
            "full mammogram images    2857\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Successfully created 'image_pathology_manifest.csv' in your Colab environment.\n",
            "You can now load this file in your next step.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Define a function to load and combine the abnormality description files\n",
        "def load_abnormal_data(base_path, mass_train, mass_test, calc_train, calc_test):\n",
        "    \"\"\"\n",
        "    Loads all four abnormality description CSVs and concatenates them.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load all CSVs\n",
        "        df_mass_train = pd.read_csv(os.path.join(base_path, mass_train))\n",
        "        df_mass_train['split'] = 'Training'\n",
        "\n",
        "        df_mass_test = pd.read_csv(os.path.join(base_path, mass_test))\n",
        "        df_mass_test['split'] = 'Test'\n",
        "\n",
        "        df_calc_train = pd.read_csv(os.path.join(base_path, calc_train))\n",
        "        df_calc_train['split'] = 'Training'\n",
        "\n",
        "        df_calc_test = pd.read_csv(os.path.join(base_path, calc_test))\n",
        "        df_calc_test['split'] = 'Test'\n",
        "\n",
        "        # Add 'abnormality_type' before concatenating\n",
        "        df_mass_train['abnormality_type'] = 'Mass'\n",
        "        df_mass_test['abnormality_type'] = 'Mass'\n",
        "        df_calc_train['abnormality_type'] = 'Calc'\n",
        "        df_calc_test['abnormality_type'] = 'Calc'\n",
        "\n",
        "        # Concatenate all into a single DataFrame\n",
        "        df_abnormal = pd.concat([df_mass_train, df_mass_test, df_calc_train, df_calc_test], ignore_index=True)\n",
        "\n",
        "        # Create the linking key for cropped images. Example: Mass-Training_P_00001_LEFT_CC_1\n",
        "        df_abnormal['PatientID_key_cropped'] = df_abnormal['abnormality_type'] + '-' + \\\n",
        "                                                df_abnormal['split'] + '_' + \\\n",
        "                                                df_abnormal['patient_id'] + '_' + \\\n",
        "                                                df_abnormal['left or right breast'] + '_' + \\\n",
        "                                                df_abnormal['image view'] + '_' + \\\n",
        "                                                df_abnormal['abnormality id'].astype(str)\n",
        "\n",
        "        # Create the linking key for full mammograms. Example: Mass-Training_P_00001_LEFT_CC\n",
        "        df_abnormal['PatientID_key_full'] = df_abnormal['abnormality_type'] + '-' + \\\n",
        "                                            df_abnormal['split'] + '_' + \\\n",
        "                                            df_abnormal['patient_id'] + '_' + \\\n",
        "                                            df_abnormal['left or right breast'] + '_' + \\\n",
        "                                            df_abnormal['image view']\n",
        "\n",
        "        return df_abnormal\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error loading file: {e}. Make sure all description CSVs are present.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while loading abnormal data: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Main execution ---\n",
        "try:\n",
        "    # Define the base path to your data in Google Drive\n",
        "    # This assumes the CSVs are in the '4600 Data' folder\n",
        "    # If they are in a subfolder, change this path.\n",
        "    gdrive_base_path = \"/Users/joshuarauf/Library/CloudStorage/GoogleDrive-jrauf7@gmail.com/My Drive/4600 Data\"\n",
        "\n",
        "    # 1. Load and process the abnormality data\n",
        "    df_abnormal = load_abnormal_data(\n",
        "        gdrive_base_path,\n",
        "        \"mass_case_description_train_set.csv\",\n",
        "        \"mass_case_description_test_set.csv\",\n",
        "        \"calc_case_description_train_set.csv\",\n",
        "        \"calc_case_description_test_set.csv\"\n",
        "    )\n",
        "\n",
        "    if df_abnormal is not None:\n",
        "        # 2. Load the main dicom_info CSV\n",
        "        dicom_info_path = os.path.join(gdrive_base_path, \"dicom_info.csv\")\n",
        "        df_dicom_info = pd.read_csv(dicom_info_path)\n",
        "\n",
        "        # 3. Create label DataFrames for merging\n",
        "\n",
        "        # Labels for cropped images\n",
        "        df_cropped_labels = df_abnormal[['PatientID_key_cropped', 'pathology']].copy()\n",
        "        df_cropped_labels.rename(columns={'PatientID_key_cropped': 'PatientID'}, inplace=True)\n",
        "        df_cropped_labels = df_cropped_labels.drop_duplicates(subset=['PatientID'])\n",
        "\n",
        "        # Labels for full images\n",
        "        df_full_labels = df_abnormal[['PatientID_key_full', 'pathology']].copy()\n",
        "        df_full_labels.rename(columns={'PatientID_key_full': 'PatientID'}, inplace=True)\n",
        "        df_full_labels = df_full_labels.drop_duplicates(subset=['PatientID'])\n",
        "\n",
        "        # 4. Merge labels onto dicom_info\n",
        "        df_merged = pd.merge(df_dicom_info, df_cropped_labels, on='PatientID', how='left')\n",
        "        df_merged = pd.merge(df_merged, df_full_labels, on='PatientID', how='left', suffixes=('_cropped', '_full'))\n",
        "\n",
        "        # 5. Consolidate pathology columns\n",
        "        df_merged['pathology'] = df_merged['pathology_cropped'].fillna(df_merged['pathology_full'])\n",
        "        df_merged.drop(columns=['pathology_cropped', 'pathology_full'], inplace=True)\n",
        "\n",
        "        # 6. Fill 'NORMAL' cases\n",
        "        # (This identifies images that are not masks and have no abnormality label)\n",
        "        image_filter = df_merged['SeriesDescription'].isin(['full mammogram images', 'cropped images'])\n",
        "        df_merged.loc[image_filter & df_merged['pathology'].isna(), 'pathology'] = 'NORMAL'\n",
        "\n",
        "        # 7. Create the final DataFrame\n",
        "        final_df = df_merged.loc[\n",
        "            image_filter & df_merged['pathology'].notna(),\n",
        "            ['image_path', 'pathology', 'SeriesDescription']\n",
        "        ].copy()\n",
        "\n",
        "        # 8. Clean the image_path to create the full, usable path\n",
        "        # 'CBIS-DDSM/jpeg/...' -> '/content/drive/MyDrive/4600 Data/jpeg/...'\n",
        "        final_df['full_image_path'] = gdrive_base_path + final_df['image_path'].str.replace('CBIS-DDSM/', '', regex=False)\n",
        "\n",
        "        # Select final columns\n",
        "        final_data_manifest = final_df[['full_image_path', 'pathology', 'SeriesDescription']]\n",
        "\n",
        "        # 9. Report and Save\n",
        "        print(\"--- Final Data Manifest Info ---\")\n",
        "        print(final_data_manifest.info())\n",
        "\n",
        "        print(\"\\n--- Value Counts for Pathology ---\")\n",
        "        print(final_data_manifest['pathology'].value_counts())\n",
        "\n",
        "        print(\"\\n--- Value Counts for Image Type ---\")\n",
        "        print(final_data_manifest['SeriesDescription'].value_counts())\n",
        "\n",
        "        # Save the manifest to your Colab session's local storage\n",
        "        output_filename = \"image_pathology_manifest.csv\"\n",
        "        final_data_manifest.to_csv(output_filename, index=False)\n",
        "        print(f\"\\nSuccessfully created '{output_filename}' in your Colab environment.\")\n",
        "        print(\"You can now load this file in your next step.\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error loading file: {e}. Please ensure 'dicom_info.csv' and the other CSVs are in '{gdrive_base_path}'\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during the main execution: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "468102d638b84992b5849edbc2a7f133",
            "49dbb3c3fb2e4745943f0e019926e743",
            "d77d47e4c4dd47e38e2c82af5f6c0b59",
            "9a24e4cb6779431989e78ae3c388e2b4",
            "2a96e35eec6c468a8d733fec18af0d17",
            "320dca0883ea48d99e3e84eca010a6d8",
            "d7e1270dc18b4885aba99e28ca6e6207",
            "dda8b9464ae546b2a57f35d4dd9e08e8",
            "302408543ae04b0280cd3af500e0ea3c",
            "ff3ee572254e481fbb1d8eab51cbb260",
            "4a05523257db4e71893e36f884c25644"
          ]
        },
        "id": "F6g8mBogJuSr",
        "outputId": "70a8119a-4795-43c1-ebf9-d54c81428d88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded manifest with 6424 entries.\n",
            "Indexing local files... this may take a moment...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# ==========================================\n",
        "# USER CONFIGURATION\n",
        "# ==========================================\n",
        "BASE_IMAGE_DIR = \"/Users/joshuarauf/Library/CloudStorage/GoogleDrive-jrauf7@gmail.com/My Drive/4600 Data/jpeg\" \n",
        "MANIFEST_PATH = \"image_pathology_manifest.csv\" \n",
        "# ==========================================\n",
        "\n",
        "# --- 1. Load Manifest ---\n",
        "if not os.path.exists(MANIFEST_PATH):\n",
        "    print(f\"Error: Manifest file not found at {MANIFEST_PATH}\")\n",
        "else:\n",
        "    df = pd.read_csv(MANIFEST_PATH)\n",
        "    print(f\"Loaded manifest with {len(df)} entries.\")\n",
        "\n",
        "# --- 2. Smart File Indexing ---\n",
        "print(\"Indexing local files... this may take a moment...\")\n",
        "\n",
        "file_map = {} \n",
        "for root, dirs, files in os.walk(BASE_IMAGE_DIR):\n",
        "    for file in files:\n",
        "        if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "            file_map[file] = os.path.join(root, file)\n",
        "\n",
        "print(f\"Found {len(file_map)} images in your local folder.\")\n",
        "\n",
        "# --- 3. Process Data ---\n",
        "if 'SeriesDescription' in df.columns:\n",
        "    df_cropped = df[df['SeriesDescription'] == 'cropped images'].copy()\n",
        "else:\n",
        "    df_cropped = df.copy()\n",
        "\n",
        "if 'pathology' in df_cropped.columns:\n",
        "    df_cropped['label'] = df_cropped['pathology'].replace({'BENIGN_WITHOUT_CALLBACK': 'BENIGN'})\n",
        "\n",
        "images = []\n",
        "labels = []\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Counters\n",
        "success_count = 0\n",
        "error_count = 0\n",
        "total_manifest_entries = 6424 \n",
        "\n",
        "print(f\"Starting image load for {len(df_cropped)} manifest entries...\")\n",
        "\n",
        "# Create a tqdm progress bar\n",
        "pbar = tqdm(df_cropped.iterrows(), total=len(df_cropped))\n",
        "\n",
        "for i, (_, row) in enumerate(pbar):\n",
        "    csv_path = row['full_image_path']\n",
        "    filename = os.path.basename(csv_path)\n",
        "    \n",
        "    if filename in file_map:\n",
        "        local_path = file_map[filename]\n",
        "        \n",
        "        try:\n",
        "            img = cv2.imread(local_path, cv2.IMREAD_GRAYSCALE)\n",
        "            \n",
        "            if img is None:\n",
        "                error_count += 1\n",
        "            else:\n",
        "                img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "                img = img / 255.0\n",
        "                \n",
        "                images.append(img)\n",
        "                labels.append(row['label'])\n",
        "                success_count += 1\n",
        "            \n",
        "        except Exception as e:\n",
        "            error_count += 1\n",
        "    else:\n",
        "        # File not found in map\n",
        "        error_count += 1\n",
        "\n",
        "    # Update the progress bar description\n",
        "    pbar.set_description(f\"Success: {success_count} | Errors: {error_count} | Processed: {i+1}/{total_manifest_entries}\")\n",
        "\n",
        "# --- 4. Finalize ---\n",
        "X = np.array(images)\n",
        "y = np.array(labels)\n",
        "\n",
        "if X.size > 0:\n",
        "    X = X.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "    print(f\"\\nSuccess!\")\n",
        "    print(f\"Images Loaded: {X.shape}\")\n",
        "    print(f\"Labels Loaded: {y.shape}\")\n",
        "    print(f\"Total Successful: {success_count}\")\n",
        "    print(f\"Total Errors/Missing: {error_count}\")\n",
        "else:\n",
        "    print(\"No images found. Please check the BASE_IMAGE_DIR path.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PL6hqai6SVdt",
        "outputId": "f70549e2-dc1e-4c2f-f527-70bac671480e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "\n",
        "# --- Prerequisite: Encode Labels ---\n",
        "# 'y' is currently an array of strings like ['BENIGN', 'MALIGNANT', ...]\n",
        "# We need to convert them to numbers\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# y_encoded is now an array of [0, 1, 1, 0, ...]\n",
        "# You can see the class mapping:\n",
        "# print(le.classes_)  # Will show ['BENIGN' 'MALIGNANT']\n",
        "\n",
        "# --- Step 1: Create the main Training (80%) and Test (20%) split ---\n",
        "# We stratify by y_encoded to ensure both splits have a similar\n",
        "# percentage of benign and malignant cases.\n",
        "TEST_SPLIT_SIZE = 0.20\n",
        "\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    X, y_encoded,\n",
        "    test_size=TEST_SPLIT_SIZE,\n",
        "    random_state=42,  # for reproducibility\n",
        "    stratify=y_encoded # This is the key part for stratification\n",
        ")\n",
        "\n",
        "# --- Step 2: Create the Training and Validation split from X_train_full ---\n",
        "# We split the 80% training data into a new training set and a validation set.\n",
        "# e.g., 0.25 * 80% = 20% validation, 75% * 80% = 60% training\n",
        "VALIDATION_SPLIT_SIZE = 0.25 # (20% of original data)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_full, y_train_full,\n",
        "    test_size=VALIDATION_SPLIT_SIZE,\n",
        "    random_state=42, # for reproducibility\n",
        "    stratify=y_train_full # Stratify this split as well\n",
        ")\n",
        "\n",
        "# --- Check your results ---\n",
        "print(f\"Original X shape: {X.shape}\")\n",
        "print(f\"Original y shape: {y_encoded.shape}\\n\")\n",
        "\n",
        "print(f\"Training X shape:   {X_train.shape}\")\n",
        "print(f\"Training y shape:   {y_train.shape}\\n\")\n",
        "\n",
        "print(f\"Validation X shape: {X_val.shape}\")\n",
        "print(f\"Validation y shape: {y_val.shape}\\n\")\n",
        "\n",
        "print(f\"Test X shape:       {X_test.shape}\")\n",
        "print(f\"Test y shape:       {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJ6kxjtkUXXi",
        "outputId": "31dbdcb0-1988-467f-b1f5-a6b04780b874"
      },
      "outputs": [],
      "source": [
        "# 1. Define the augmentation layers\n",
        "# These will run on the GPU, making them very fast.\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    # Add 'input_shape' only if this is the first layer of your model\n",
        "    # tf.keras.layers.InputLayer(input_shape=(IMG_SIZE, IMG_SIZE, 1)),\n",
        "\n",
        "    # --- random rotations ---\n",
        "    tf.keras.layers.RandomRotation(0.1),  # rotate by +/- 10%\n",
        "\n",
        "    # --- horizontal flips ---\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "\n",
        "    # --- scaling (zoom) ---\n",
        "    tf.keras.layers.RandomZoom(0.1), # zoom in/out by +/- 10%\n",
        "\n",
        "    # --- slight adjustments in brightness and contrast ---\n",
        "    tf.keras.layers.RandomBrightness(0.1), # adjust brightness by +/- 10%\n",
        "    tf.keras.layers.RandomContrast(0.1)  # adjust contrast by +/- 10%\n",
        "], name=\"data_augmentation\")\n",
        "\n",
        "\n",
        "# 2. Build your tf.data pipelines\n",
        "# This is the modern, efficient way to feed data to a model\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Create datasets from your NumPy arrays\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "\n",
        "# 3. Apply augmentation ONLY to the training set\n",
        "# We also batch and prefetch for performance\n",
        "train_ds = train_ds.shuffle(buffer_size=len(X_train)) \\\n",
        "                   .batch(BATCH_SIZE) \\\n",
        "                   .map(lambda x, y: (data_augmentation(x, training=True), y),\n",
        "                        num_parallel_calls=tf.data.AUTOTUNE) \\\n",
        "                   .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# Validation and Test sets: DO NOT augment. Just batch and prefetch.\n",
        "val_ds = val_ds.batch(BATCH_SIZE) \\\n",
        "               .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "test_ds = test_ds.batch(BATCH_SIZE) \\\n",
        "                 .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "print(\"\\nSuccessfully created tf.data pipelines.\")\n",
        "print(f\"Training dataset:   {train_ds}\")\n",
        "print(f\"Validation dataset: {val_ds}\")\n",
        "print(f\"Test dataset:       {test_ds}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyIuOsexUzz4",
        "outputId": "8d584763-00b8-438c-c15e-5ec3b68af195"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# --- 1. Flatten the Data ---\n",
        "# Our sklearn models need 2D data (samples, features), not 4D (samples, h, w, c)\n",
        "# We flatten (224, 224, 1) into a single vector of 50,176 features\n",
        "\n",
        "print(\"Flattening data for sklearn models...\")\n",
        "# Get the number of samples\n",
        "n_samples_train = X_train.shape[0]\n",
        "n_samples_val = X_val.shape[0]\n",
        "\n",
        "# Reshape\n",
        "X_train_flat = X_train.reshape(n_samples_train, -1)\n",
        "X_val_flat = X_val.reshape(n_samples_val, -1)\n",
        "\n",
        "print(f\"Original X_train shape: {X_train.shape}\")\n",
        "print(f\"New X_train_flat shape: {X_train_flat.shape}\")\n",
        "print(f\"Original X_val shape: {X_val.shape}\")\n",
        "print(f\"New X_val_flat shape: {X_val_flat.shape}\\n\")\n",
        "\n",
        "# --- 2. Model 1: Logistic Regression ---\n",
        "# We use a Pipeline to chain scaling and the model.\n",
        "# Logistic Regression benefits from scaling and needs more iterations\n",
        "# to converge on this many features.\n",
        "print(\"--- Training: Logistic Regression ---\")\n",
        "# We use StandardScaler for better performance\n",
        "lr_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', LogisticRegression(max_iter=1000, random_state=42))\n",
        "])\n",
        "\n",
        "lr_pipeline.fit(X_train_flat, y_train)\n",
        "\n",
        "# Evaluate on the validation set\n",
        "y_pred_lr = lr_pipeline.predict(X_val_flat)\n",
        "print(\"Logistic Regression - Validation Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_val, y_pred_lr):.4f}\")\n",
        "print(classification_report(y_val, y_pred_lr, target_names=le.classes_))\n",
        "\n",
        "\n",
        "# --- 3. Model 2: k-Nearest Neighbors (kNN) ---\n",
        "# kNN is very sensitive to scaling and the 'curse of dimensionality'\n",
        "# (which we have with 50k+ features!), so it may be slow and/or inaccurate.\n",
        "print(\"--- Training: k-Nearest Neighbors (kNN) ---\")\n",
        "knn_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', KNeighborsClassifier(n_neighbors=5)) # Using k=5 as a default\n",
        "])\n",
        "\n",
        "knn_pipeline.fit(X_train_flat, y_train)\n",
        "\n",
        "# Evaluate on the validation set\n",
        "y_pred_knn = knn_pipeline.predict(X_val_flat)\n",
        "print(\"kNN - Validation Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_val, y_pred_knn):.4f}\")\n",
        "print(classification_report(y_val, y_pred_knn, target_names=le.classes_))\n",
        "\n",
        "\n",
        "# --- 4. Model 3: Gaussian Naive Bayes ---\n",
        "# This model makes strong independence assumptions about features,\n",
        "# which is not true for pixels, but it's a very fast baseline.\n",
        "# It doesn't require scaling.\n",
        "print(\"--- Training: Gaussian Naive Bayes ---\")\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train_flat, y_train)\n",
        "\n",
        "# Evaluate on the validation set\n",
        "y_pred_gnb = gnb.predict(X_val_flat)\n",
        "print(\"Gaussian Naive Bayes - Validation Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_val, y_pred_gnb):.4f}\")\n",
        "print(classification_report(y_val, y_pred_gnb, target_names=le.classes_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rla3u15FWuB0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# --- 1. Create a Custom PyTorch Dataset ---\n",
        "# This class will handle our numpy arrays and convert them to\n",
        "# the (N, C, H, W) format PyTorch expects.\n",
        "\n",
        "class MammoDataset(Dataset):\n",
        "    def __init__(self, images, labels):\n",
        "        # images are (N, H, W, C) - we need (N, C, H, W)\n",
        "        # We permute the dimensions here: (0, 3, 1, 2)\n",
        "        # (N, H, W, C) -> (N, C, H, W)\n",
        "        self.X = torch.tensor(images, dtype=torch.float32).permute(0, 3, 1, 2)\n",
        "        # Labels for CrossEntropyLoss need to be LongTensors\n",
        "        self.y = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# --- 2. Define the Baseline CNN Architecture ---\n",
        "\n",
        "class BaseCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaseCNN, self).__init__()\n",
        "        # Input shape: (Batch, 1, 224, 224)\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        # Shape: (Batch, 16, 224, 224)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # Shape: (Batch, 16, 112, 112)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        # Shape: (Batch, 32, 112, 112)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # Shape: (Batch, 32, 56, 56)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        # Shape: (Batch, 64, 56, 56)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # Shape: (Batch, 64, 28, 28)\n",
        "\n",
        "        # Flatten the output for the fully-connected layers\n",
        "        # 64 channels * 28 * 28 = 50,176\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 28 * 28, 512)\n",
        "        self.relu4 = nn.ReLU()\n",
        "\n",
        "        # Output layer: 2 classes (BENIGN, MALIGNANT)\n",
        "        self.fc2 = nn.Linear(512, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = self.pool3(self.relu3(self.conv3(x)))\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu4(self.fc1(x))\n",
        "        x = self.fc2(x) # No softmax here, CrossEntropyLoss handles it\n",
        "        return x\n",
        "\n",
        "# --- 3. Set up Data, Model, Loss, and Optimizer ---\n",
        "\n",
        "# Set device (use GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\\n\")\n",
        "\n",
        "# Create Datasets\n",
        "# X_train, y_train, X_val, y_val are from your sklearn train_test_split\n",
        "train_dataset = MammoDataset(X_train, y_train)\n",
        "val_dataset = MammoDataset(X_val, y_val)\n",
        "\n",
        "# Create DataLoaders\n",
        "BATCH_SIZE = 32\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "model = BaseCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss() # Handles softmax internally\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(\"Model architecture:\")\n",
        "print(model)\n",
        "print(\"\\nStarting training...\")\n",
        "\n",
        "# --- 4. Training and Validation Loop ---\n",
        "NUM_EPOCHS = 10 # You can increase this\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # --- Training Phase ---\n",
        "    model.train() # Set model to training mode\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Move data to the device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # 1. Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 2. Forward pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        # 3. Calculate loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # 4. Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "\n",
        "    train_loss = train_loss / len(train_loader.dataset)\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    val_corrects = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad(): # No gradients needed for validation\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Get predictions (class with the highest score)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            val_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_loss = val_loss / len(val_loader.dataset)\n",
        "    val_acc = val_corrects.double() / len(val_loader.dataset)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
        "          f\"Train Loss: {train_loss:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "# --- 5. Final Evaluation ---\n",
        "print(\"\\nTraining complete.\")\n",
        "print(\"Final Validation Results:\")\n",
        "# 'le' is the LabelEncoder you fit earlier\n",
        "print(classification_report(all_labels, all_preds, target_names=le.classes_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "70fd1c03d57a4e4fb3c0f4a12893e265",
            "7e02d926e86845f0ba5d7086f260eb08",
            "005ea857a153497c8cc9da53e05f6593",
            "62e5d60383104a8ba3da202881e99078",
            "041d88dc849b4f4bbbaa8df5c1a054a2",
            "f5f56a7ccc2444b1b36a7365bacc679b",
            "48d0ad6efcc94d179a6399cff4257904",
            "4caee4a9ec4449f4bfdefa9622e58994",
            "24e48e60285f480abf05e4abc5a3c570",
            "7f57de839cbf41b99c2715191cea99e5",
            "3164fbfb344b4216bc56b5c76a094946"
          ]
        },
        "id": "p0ahpJwoD6I4",
        "outputId": "c5f43caf-ff00-433e-a64a-cdc8151f5c45"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import torchvision.models as models\n",
        "\n",
        "# --- 1. Create a Custom PyTorch Dataset ---\n",
        "# We must modify this Dataset to handle 3 channels, as ResNet\n",
        "# expects a 3-channel (RGB) input.\n",
        "\n",
        "class MammoDataset(Dataset):\n",
        "    def __init__(self, images, labels):\n",
        "        # images are (N, H, W, C) - we need (N, C, H, W)\n",
        "        images_permuted = torch.tensor(images, dtype=torch.float32).permute(0, 3, 1, 2)\n",
        "\n",
        "        # --- NEW: Duplicate the single channel 3 times ---\n",
        "        # (N, 1, H, W) -> (N, 3, H, W)\n",
        "        self.X = images_permuted.repeat(1, 3, 1, 1)\n",
        "\n",
        "        self.y = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# --- 2. Define the Pre-trained Model ---\n",
        "\n",
        "# Load a ResNet-50 model pre-trained on ImageNet\n",
        "# NEW\n",
        "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "\n",
        "# We need to replace the final layer (the \"head\")\n",
        "# The original model was trained on 1000 classes. We only have 2.\n",
        "num_features = model.fc.in_features # Get size of features before the last layer\n",
        "\n",
        "# Replace the final layer with a new, untrained layer for our 2 classes\n",
        "model.fc = nn.Linear(num_features, 2)\n",
        "\n",
        "# --- 3. Set up Data, Model, Loss, and Optimizer ---\n",
        "\n",
        "# Set device (THIS MUST BE 'cuda' AFTER YOU ENABLE THE GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\\n\")\n",
        "\n",
        "# Move the model to the GPU\n",
        "model = model.to(device)\n",
        "\n",
        "# Create Datasets and DataLoaders\n",
        "# (Assumes X_train, y_train, etc. are in memory)\n",
        "train_dataset = MammoDataset(X_train, y_train)\n",
        "val_dataset = MammoDataset(X_val, y_val)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# We only want to train the *new* final layer (model.fc) at first,\n",
        "# or we can train all parameters with a low learning rate.\n",
        "# Let's start by fine-tuning the whole model.\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001) # Use a much smaller LR\n",
        "\n",
        "print(\"Model architecture: ResNet-50 (Fine-Tuned)\")\n",
        "print(\"Starting training...\")\n",
        "\n",
        "# --- 4. Training and Validation Loop ---\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    # Add a progress bar using tqdm\n",
        "    from tqdm.notebook import tqdm\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\"):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "\n",
        "    train_loss = train_loss / len(train_loader.dataset)\n",
        "\n",
        "    # --- Validation ---\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_corrects = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Val]\"):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            val_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_loss = val_loss / len(val_loader.dataset)\n",
        "    val_acc = val_corrects.double() / len(val_loader.dataset)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
        "          f\"Train Loss: {train_loss:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "# --- 5. Final Evaluation ---\n",
        "print(\"\\nTraining complete.\")\n",
        "print(\"Final Validation Results (ResNet-50 Fine-Tuned):\")\n",
        "print(classification_report(all_labels, all_preds, target_names=le.classes_))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "4600env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "005ea857a153497c8cc9da53e05f6593": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4caee4a9ec4449f4bfdefa9622e58994",
            "max": 67,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_24e48e60285f480abf05e4abc5a3c570",
            "value": 3
          }
        },
        "041d88dc849b4f4bbbaa8df5c1a054a2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24e48e60285f480abf05e4abc5a3c570": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2a96e35eec6c468a8d733fec18af0d17": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "302408543ae04b0280cd3af500e0ea3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3164fbfb344b4216bc56b5c76a094946": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "320dca0883ea48d99e3e84eca010a6d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "468102d638b84992b5849edbc2a7f133": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_49dbb3c3fb2e4745943f0e019926e743",
              "IPY_MODEL_d77d47e4c4dd47e38e2c82af5f6c0b59",
              "IPY_MODEL_9a24e4cb6779431989e78ae3c388e2b4"
            ],
            "layout": "IPY_MODEL_2a96e35eec6c468a8d733fec18af0d17"
          }
        },
        "48d0ad6efcc94d179a6399cff4257904": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49dbb3c3fb2e4745943f0e019926e743": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_320dca0883ea48d99e3e84eca010a6d8",
            "placeholder": "",
            "style": "IPY_MODEL_d7e1270dc18b4885aba99e28ca6e6207",
            "value": "Loadingimages:100%"
          }
        },
        "4a05523257db4e71893e36f884c25644": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4caee4a9ec4449f4bfdefa9622e58994": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62e5d60383104a8ba3da202881e99078": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f57de839cbf41b99c2715191cea99e5",
            "placeholder": "",
            "style": "IPY_MODEL_3164fbfb344b4216bc56b5c76a094946",
            "value": "3/67[00:20&lt;07:12,6.76s/it]"
          }
        },
        "70fd1c03d57a4e4fb3c0f4a12893e265": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7e02d926e86845f0ba5d7086f260eb08",
              "IPY_MODEL_005ea857a153497c8cc9da53e05f6593",
              "IPY_MODEL_62e5d60383104a8ba3da202881e99078"
            ],
            "layout": "IPY_MODEL_041d88dc849b4f4bbbaa8df5c1a054a2"
          }
        },
        "7e02d926e86845f0ba5d7086f260eb08": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5f56a7ccc2444b1b36a7365bacc679b",
            "placeholder": "",
            "style": "IPY_MODEL_48d0ad6efcc94d179a6399cff4257904",
            "value": "Epoch1/10[Train]:4%"
          }
        },
        "7f57de839cbf41b99c2715191cea99e5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a24e4cb6779431989e78ae3c388e2b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff3ee572254e481fbb1d8eab51cbb260",
            "placeholder": "",
            "style": "IPY_MODEL_4a05523257db4e71893e36f884c25644",
            "value": "3567/3567[45:36&lt;00:00,1.12it/s]"
          }
        },
        "d77d47e4c4dd47e38e2c82af5f6c0b59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dda8b9464ae546b2a57f35d4dd9e08e8",
            "max": 3567,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_302408543ae04b0280cd3af500e0ea3c",
            "value": 3567
          }
        },
        "d7e1270dc18b4885aba99e28ca6e6207": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dda8b9464ae546b2a57f35d4dd9e08e8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5f56a7ccc2444b1b36a7365bacc679b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff3ee572254e481fbb1d8eab51cbb260": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
